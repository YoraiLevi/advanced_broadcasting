{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation over dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# mat is m x n matrix\n",
    "mat = \\\n",
    "[[1,2,3],\n",
    " [4,5,6],\n",
    " [7,8,9],\n",
    " [10,11,12]]\n",
    "t = torch.Tensor(mat)\n",
    "i = 1 # i is rows, (i in [m])\n",
    "j = 2 # j is columns (j in [n])\n",
    "\n",
    "display(mat[i][j])\n",
    "display(t[i][j])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*jmXqsVUNaBaUsBAkHgqb3A.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5, 6, 8])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(t.shape)\n",
    "# sum over rows, iterate over i's \\sigma_{i=1}^m t_{ij}\n",
    "# [1+4+7+10, 2+5+8+11, 3+6+9+12] = [22, 26, 30]\n",
    "display(t.sum(dim=0).shape) # we \"lost\" the 0th dimension\n",
    "# sum over columns, iterate over j's \\sigma_{j=1}^n t_{ij}\n",
    "# [1+2+3 , 4+5+6, 7+8+9, 10+11+12] = [6, 15, 24, 33]\n",
    "display(t.sum(dim=1).shape) # we \"lost\" the 1st dimension\n",
    "\n",
    "display(torch.rand(4,5,6,7,8).sum(dim=3).shape) # we \"lost\" the 3rd dimension, which had size 7"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with reading numpy's explanation of broadcasting: https://numpy.org/doc/stable/user/basics.broadcasting.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Broadcasting Rules\n",
    "When operating on two arrays, NumPy compares their shapes element-wise. It starts with the trailing (i.e. rightmost) dimension and works its way left. Two dimensions are compatible when\n",
    "\n",
    "1) they are equal, or\n",
    "\n",
    "2) one of them is 1.\n",
    "\n",
    "If these conditions are not met, a ValueError: operands could not be broadcast together exception is thrown, indicating that the arrays have incompatible shapes.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy's explaination is not really comprehensive enough and even reading the entire article doesn't provide enough intuition to convert \"scientific formula\" into simple broadcasted operations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check this article:\n",
    "https://towardsdatascience.com/broadcasting-in-numpy-58856f926d73\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*h8hFhsht_xbQihRV8fzKAg.png)\n",
    "\n",
    "Rule 1: If the two arrays differ in their number of dimensions, the shape of the one with fewer dimensions is padded with ones on its leading (left) side.\n",
    "\n",
    "Rule 2: If the shape of the two arrays does not match in any dimension, the array with shape equal to 1 in that dimension is stretched to match the other shape.\n",
    "\n",
    "Rule 3: If in any dimension the sizes disagree and neither is equal to 1, an error is raised.\n",
    "\n",
    "Stretching means that the corresponding layers are replicated:  \n",
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*3owRZj2AolN6Ry45bOKskg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[7],\n",
       "        [8]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[5, 6, 7]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = torch.arange(2*3).reshape(2,3)\n",
    "V = torch.arange(2).reshape(2,1)+7\n",
    "U = torch.arange(3).reshape(1,3)+5\n",
    "display(A.shape, A)\n",
    "display(V.shape, V)\n",
    "display(U.shape, U)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following tensors are compatible for broadcasting\n",
    "\n",
    "A [2 3]  \n",
    "V [2 1]   - dimension is 1 and then equals to A's  \n",
    "U [1 3]   - dimension equals to A's and then is 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[5, 6, 7]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 5,  7,  9],\n",
       "        [ 8, 10, 12]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 7,  8,  9],\n",
       "        [11, 12, 13]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[7],\n",
       "        [8]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(A)\n",
    "display(U)\n",
    "display(A+U)\n",
    "display(A+V)\n",
    "display(V)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy also provides a function called np.broadcast_to, which allows us to explicitly see the broadcasted shape without any computation or tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(torch.broadcast_shapes((2,3), (2,1), (1,3)))\n",
    "display(torch.broadcast_shapes(A.shape, V.shape, U.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4, 3, 2) -- (5, 4, 3, 2)\n",
      "       (, 1) -> (:, :, :, :)\n",
      "       (, 2) -> (:, :, :, 2)\n",
      "      (3, 2) -> (:, :, 3, 2)\n",
      "   (4, 1, 2) -> (:, 4, :, 2)\n",
      "(5, 4, 3, 2) -> (5, 4, 3, 2)\n",
      "----\n",
      "(2, 3) -- (2, 3)\n",
      "(2, 1) -> (2, :)\n",
      "(1, 3) -> (:, 3)\n"
     ]
    }
   ],
   "source": [
    "#Broadcast print!\n",
    "def print_broadcast_shaped(*shapes):\n",
    "    try:\n",
    "        final_shape = tuple(torch.broadcast_shapes(*shapes))\n",
    "    except:\n",
    "        print(\"Shapes cannot be broadcast together.\")\n",
    "        return None\n",
    "    # print the resulting broadcasted shape first\n",
    "    print(f\"{str(final_shape):>{len(str(final_shape))}}\",'--',final_shape)\n",
    "    for shape in shapes:\n",
    "        left_prefix = ':, '*(len(final_shape)-len(shape))\n",
    "        # we know that the shapes are broadcastable, so we can assume the tuples are equal\n",
    "        right_shape = ', '.join(':' if shape[i] == 1 else str(shape[i]) for i in (range(len(shape))))\n",
    "        pretty_shape = f\"(, {shape[0]})\" if len(shape) == 1 else str(shape)\n",
    "        print(f\"{pretty_shape:>{len(str(final_shape))}}\",'->','('+left_prefix+right_shape+')')\n",
    "\n",
    "print_broadcast_shaped(\n",
    "    (1,),\n",
    "    (2,),\n",
    "    (3,2),\n",
    "    (4,1,2),\n",
    "    (5,4,3,2),\n",
    "    )\n",
    "print('----')\n",
    "print_broadcast_shaped(\n",
    "    (2,1),\n",
    "    (1,3),\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this pretty print out we can see the dimension the broadcasting \"duplicates\" our data on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[5, 6, 7]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[5., 6., 7.],\n",
       "        [5., 6., 7.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[7., 7., 7.],\n",
       "        [8., 8., 8.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[7],\n",
       "        [8]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "zeros = torch.zeros(2,3)\n",
    "display(zeros)\n",
    "display(U)\n",
    "display(zeros+U)\n",
    "display(zeros+V)\n",
    "display(V)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.wikiwand.com/en/Yorick_(programming_language) - origins of broadcasting?\n",
    "https://www.wikiwand.com/en/Perl_Data_Language\n",
    "\n",
    "comprehensive numpy tutorial\n",
    "https://betterprogramming.pub/numpy-illustrated-the-visual-guide-to-numpy-3b1d4976de1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([7, 8])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[5, 6, 7]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsqueeze(1):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[7],\n",
       "        [8]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 7,  8,  9],\n",
       "        [11, 12, 13]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"A:\")\n",
    "display(A)\n",
    "Vrange = torch.arange(2)+7\n",
    "display(Vrange)\n",
    "print('U:')\n",
    "display(U)\n",
    "print('unsqueeze(1):')\n",
    "display(Vrange.unsqueeze(1))\n",
    "display(A+Vrange.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([5, 6, 7])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[5, 6, 7]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsqueeze(0):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[5, 6, 7]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 5,  7,  9],\n",
       "        [ 8, 10, 12]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"A:\")\n",
    "display(A)\n",
    "Urange = torch.arange(3)+5\n",
    "display(Urange)\n",
    "print('U:')\n",
    "display(U)\n",
    "print('unsqueeze(0):')\n",
    "display(Urange.unsqueeze(0))\n",
    "display(A+Urange.unsqueeze(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[12, 14, 16],\n",
       "        [16, 18, 20]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[12, 14, 16],\n",
       "        [16, 18, 20]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# How would one reproduce the following matrix using broadcasting?\n",
    "#          [ |  |  |  ]   [--U--]\n",
    "# A* = A + [ V   V  V ] + [--U--]\n",
    "#          [ |  |  | ]    [--U--]\n",
    "# A[mxn] - matrix\n",
    "# V[mx1] - col\n",
    "# U[1xn] - row\n",
    "import itertools\n",
    "\n",
    "\n",
    "display(A.shape,V.shape,U.shape)\n",
    "Astar = A + V + U\n",
    "display(Astar)\n",
    "print('------------------')\n",
    "# For i,j in A:\n",
    "#    A*[i,j] = A[i,j] + V[i] + U[j]\n",
    "Astar_manual = torch.zeros_like(A)\n",
    "display(A.shape,Vrange.shape,Urange.shape)\n",
    "for (i,j) in itertools.product(range(A.shape[0]),range(A.shape[1])):\n",
    "    # Astar_manual[i,j] = A[i,j] + V[i,:] + U[:,j] # these are already unqueezed in ':' dimension\n",
    "    Astar_manual[i,j] = A[i,j] + Vrange[i] + Urange[j] # these are unqueezed in : dimension\n",
    "display(Astar_manual)\n",
    "# Whenever you want to to an V[i,:] (Vrange[i]) or U[:,j] (Urange[j]) you need to unsqueeze (have it be 1) the ':' dimension\n",
    "# torch.Size([2, 1]) | V[i,:] | Vrange[i]\n",
    "# torch.Size([1, 3]) | U[:,j] | Urange[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  1.,  3.,  6., 10.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3., 4., 0.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4., 0.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "unsupported operation: some elements of the input tensor and the written-to tensor refer to a single memory location. Please clone() the tensor before performing the operation.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[182], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m display(t)\n\u001b[0;32m     12\u001b[0m display(strided)\n\u001b[1;32m---> 13\u001b[0m t \u001b[39m+\u001b[39;49m\u001b[39m=\u001b[39;49m t[\u001b[39m1\u001b[39;49m]\n\u001b[0;32m     14\u001b[0m \u001b[39m# t +=strided\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m# t\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: unsupported operation: some elements of the input tensor and the written-to tensor refer to a single memory location. Please clone() the tensor before performing the operation."
     ]
    }
   ],
   "source": [
    "# Beyond the 2D!\n",
    "# Beyond the 3D!\n",
    "# Beyond the Time Dimension! - Can broadcasting be used for time series calculations?\n",
    "# cumsum using broadcasting\n",
    "display(torch.arange(5.).cumsum(dim=0))\n",
    "t = torch.arange(6.)\n",
    "\n",
    "# https://www.wikiwand.com/en/Summed_area_table\n",
    "t[-1] = 0\n",
    "strided = t.as_strided(size=(5,),stride=(1,),storage_offset=1)\n",
    "display(t)\n",
    "display(strided)\n",
    "t += t[1]\n",
    "# t +=strided\n",
    "# t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.,  8., 10.],\n",
       "       [ 9., 11., 13.]])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# array = np.arange(6.)\n",
    "array = np.arange(6.).reshape(2,3) + 3\n",
    "# array([[3., 4., 5.],\n",
    "#       [6., 7., 8.]])\n",
    "s = np.lib.stride_tricks.as_strided(array, shape=(1,3)) # array([[3., 4., 5.]])\n",
    "array += s\n",
    "array\n",
    "# array([[ 6.,  8., 10.],\n",
    "#        [ 9., 11., 13.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "unsupported operation: some elements of the input tensor and the written-to tensor refer to a single memory location. Please clone() the tensor before performing the operation.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[243], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39m# tensor([[3., 4., 5.],\u001b[39;00m\n\u001b[0;32m      4\u001b[0m         \u001b[39m# [6., 7., 8.]])\u001b[39;00m\n\u001b[0;32m      5\u001b[0m s \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mas_strided(size\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m3\u001b[39m),stride\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m)) \u001b[39m# tensor([[3., 4., 5.]])\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m t \u001b[39m+\u001b[39;49m\u001b[39m=\u001b[39;49m s\n\u001b[0;32m      7\u001b[0m t\n\u001b[0;32m      8\u001b[0m \u001b[39m# RuntimeError                              Traceback (most recent call last)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39m#       3 # tensor([[3., 4., 5.],\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m#       4         # [6., 7., 8.]])\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[39m# RuntimeError: unsupported operation: some elements of the input tensor and the written-to tensor refer to a single memory location. Please clone() the tensor before performing the operation.\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: unsupported operation: some elements of the input tensor and the written-to tensor refer to a single memory location. Please clone() the tensor before performing the operation."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "t = torch.arange(6.).reshape(2,3) + 3\n",
    "# tensor([[3., 4., 5.],\n",
    "        # [6., 7., 8.]])\n",
    "s = t.as_strided(size=(1,3),stride=(1,1)) # tensor([[3., 4., 5.]])\n",
    "t += s\n",
    "t\n",
    "# RuntimeError                              Traceback (most recent call last)\n",
    "#       3 # tensor([[3., 4., 5.],\n",
    "#       4         # [6., 7., 8.]])\n",
    "#       5 s = t.as_strided(size=(1,3),stride=(1,1)) # tensor([[3., 4., 5.]])\n",
    "# ----> 6 t += s\n",
    "#       7 t\n",
    "\n",
    "# RuntimeError: unsupported operation: some elements of the input tensor and the written-to tensor refer to a single memory location. Please clone() the tensor before performing the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.,  8., 10.],\n",
       "       [ 9., 11., 13.]])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array += np.lib.stride_tricks.as_strided(array, shape=(1,3))\n",
    "array"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/q/56731361/12603110\n",
    "In this question the guy requests a somewhat interesting task for combining matrices.\n",
    "Lets investigate the problem and see if we can find a solution that uses broadcasting\n",
    "```python\n",
    "R = np.zeros([D,D])\n",
    "\n",
    "for a in range(D):\n",
    "  for b in range(D):\n",
    "    for c in range(D):\n",
    "      if 0 <= b+c-a < D:\n",
    "        R[a, b+c-a] += T[a, b, c]*M[b, c]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,  10.,  56.],\n",
       "        [ 46., 164., 182.],\n",
       "        [272., 290., 208.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "D = 3\n",
    "M = torch.arange(D*D).reshape(D,D)\n",
    "T = torch.arange(D*D*D).reshape(D,D,D)\n",
    "\n",
    "R = torch.zeros([D,D])\n",
    "\n",
    "for a in range(D):\n",
    "  for b in range(D):\n",
    "    for c in range(D):\n",
    "      if 0 <= b+c-a < D:\n",
    "        R[a, b+c-a] += T[a, b, c]*M[b, c]\n",
    "display(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2],\n",
       "         [ 3,  4,  5],\n",
       "         [ 6,  7,  8]],\n",
       "\n",
       "        [[ 9, 10, 11],\n",
       "         [12, 13, 14],\n",
       "         [15, 16, 17]],\n",
       "\n",
       "        [[18, 19, 20],\n",
       "         [21, 22, 23],\n",
       "         [24, 25, 26]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5],\n",
       "        [6, 7, 8]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  4],\n",
       "        [ 9, 16, 25],\n",
       "        [36, 49, 64]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  0,  10,  22],\n",
       "        [ 36,  52,  70],\n",
       "        [ 90, 112, 136]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  0,  19,  40],\n",
       "        [ 63,  88, 115],\n",
       "        [144, 175, 208]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  2,  4],\n",
       "         [ 6,  8, 10],\n",
       "         [12, 14, 16]],\n",
       "\n",
       "        [[ 9, 11, 13],\n",
       "         [15, 17, 19],\n",
       "         [21, 23, 25]],\n",
       "\n",
       "        [[18, 20, 22],\n",
       "         [24, 26, 28],\n",
       "         [30, 32, 34]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's first solve the multiplication of T[a,b,c] * M[b,c]\n",
    "# T[a,b,c] * M[b,c]\n",
    "# T[a,b,c] * M[:,b,c]\n",
    "display(T,M)\n",
    "display(T[0] * M,T[1] * M,T[2] * M)\n",
    "(T * M)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caveats and gotchas\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, there’s a total of three types of vectors in NumPy: 1D arrays, 2D row vectors, and 2D column vectors. Here’s a diagram of explicit conversions between those:  \n",
    "https://betterprogramming.pub/numpy-illustrated-the-visual-guide-to-numpy-3b1d4976de1d\n",
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*nVqN9Ha2tBO4isoGqCIsMQ.png)  \n",
    "* flatten is always a copy\n",
    "* ravel() is usually a view\n",
    "* reshape(-1) is a view when possible\n",
    "* `.shape=` is always a view   \n",
    "(see “[5 ways to flatten an array](https://betterprogramming.pub/5-ways-to-flatten-an-array-in-numpy-dd6d79042139)” for details)  \n",
    "\n",
    "By the rules of broadcasting, 1D arrays are implicitly interpreted as 2D row vectors, so it is generally not necessary to convert between those two — thus the corresponding area is shaded.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Reading\n",
    "einsum: https://betterprogramming.pub/einsum-visualized-c050903145ef"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
